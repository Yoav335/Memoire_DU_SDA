import streamlit as st
import pandas as pd
from utils.data_loader import load_data
from utils.ml_models import train_ridge, train_random_forest, train_ridge_split
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
import seaborn as sns

st.title("ü§ñ Machine Learning ‚Äì Analyse des √©missions et croissance")

# Charger les donn√©es
df = load_data()

# D√©finir les colonnes disponibles
# Cibles : PIB + CO2
# Cr√©er les colonnes
df['gdp_per_capita'] = df['gdp'] / df['population']
df['co2_per_capita'] = df['co2'] / df['population']

# D√©finir les cibles par nom de colonne
target_vars = ['gdp', 'gdp_per_capita', 'co2', 'co2_per_capita']


# Supprimer colonnes cibles, pays, ann√©e
feature_vars = [c for c in df.columns if c not in target_vars + ['country', 'year']]

# Variables explicatives par d√©faut
#default_features = [
   # "cement_co2",
   # "cumulative_cement_co2",
   # "oil_co2",
   # "co2_growth_prct",
   # "co2_including_luc_growth_abs", -0,2
    #"coal_co2",
   # "co2_including_luc_per_unit_energy",
   # "methane",
    #"co2_growth_abs",
   # "nitrous_oxide",
   # "co2_including_luc",
   # "co2_including_luc_per_gdp",
    #"co2_including_luc_growth_prct",
    # "energy_per_gdp"

    # Pour la belegique, on test d'autres variables
default_features = [
    "cement_co2",
    "oil_co2",
    "coal_co2",
    "methane",
    "nitrous_oxide",
    
    # Intensit√© carbone / √©nergie
   # "co2_including_luc_per_unit_energy",  # gCO2 par unit√© d'√©nergie
   # "energy_per_capita",                  # √©nergie par habitant
    #"energy_per_gdp",                     # √©nergie par unit√© de PIB
    
    # Variation et tendance des √©missions
    #"co2_growth_abs",
    #"co2_growth_prct",
    #"co2_including_luc_growth_abs",
    #"co2_including_luc_growth_prct",
    
    # Autres indicateurs √©conomiques propres
   # "gdp_per_capita",                     # pour capturer effet taille/population
]

# S√©lection de la target et des features
target = st.selectbox("Choisissez la variable cible (target)", target_vars)
features = st.multiselect(
    "S√©lectionnez les variables explicatives",
    feature_vars,
    default=[f for f in default_features if f in feature_vars]
)

# S√©lection de la zone
pays_disponibles = sorted(df['country'].dropna().unique())
zones = ["Monde"] + pays_disponibles
zone = st.selectbox("Choisissez la zone g√©ographique (Personnalis√©)", zones, key="zone_custom")

# Choix du mod√®le ML
model_choice = st.radio(
    "Choisissez le mod√®le de Machine Learning",
    ["Ridge Regression (train only)", "Ridge Regression (train/test)", "Random Forest"]
)

# Tabs pour organisation
ml_tabs = st.tabs(["üìö Entra√Ænement", "üìä Graphiques", "üìà Comparaison"])

# Stockage des r√©sultats
if "ml_results" not in st.session_state:
    st.session_state.ml_results = {}

# --- Onglet Entra√Ænement ---
with ml_tabs[0]:
    st.subheader("Entra√Ænement du mod√®le")
    if st.button("Lancer l'entra√Ænement"):
        if not features:
            st.error("Veuillez s√©lectionner au moins une variable explicative.")
        else:
            # Filtrer le dataframe selon la zone s√©lectionn√©e
            df_zone = df.copy() if zone == "Monde" else df[df["country"] == zone].copy()

            if model_choice == "Ridge Regression (train only)":
                results = train_ridge(df_zone, target, features)
            elif model_choice == "Ridge Regression (train/test)":
                results = train_ridge_split(df_zone, target, features)
            elif model_choice == "Random Forest":
                results = train_random_forest(df_zone, target, features)

            model_key = f"{model_choice} - {target} - {zone}"
            st.session_state.ml_results[model_key] = results
            st.success(f"Mod√®le {model_choice} entra√Æn√© avec succ√®s pour la zone {zone} !")

# --- Onglet Graphiques ---
with ml_tabs[1]:
    st.subheader("Visualisation des r√©sultats")
    if st.session_state.ml_results:
        model_selected = st.selectbox("Choisissez un mod√®le entra√Æn√©", list(st.session_state.ml_results.keys()))
        res = st.session_state.ml_results[model_selected]

        # Pour Ridge train/test, on utilise y_test et y_pred_test
        if "y_test" in res and "y_pred_test" in res:
            from utils.ml_models import plot_real_vs_pred, plot_feature_importance
            y = res["y_test"]
            y_pred = res["y_pred_test"]

            # Graph valeurs r√©elles vs pr√©dictions
            plot_real_vs_pred(y, y_pred, f"{model_selected} : Valeurs r√©elles vs Pr√©dictions")

            # Cas Ridge (coefficients dispo)
            if "coefficients" in res:
                coeffs = pd.Series(res["coefficients"]).sort_values(ascending=False)
                plot_feature_importance(coeffs, f"{model_selected} : Coefficients")

                # ‚úÖ Tableau des coefficients
                st.markdown("### üìã Coefficients des variables")
                st.dataframe(coeffs.to_frame("Coefficient"))

            # Cas Random Forest (feature importances dispo)
            elif "feature_importances" in res:
                importances = pd.Series(res["feature_importances"]).sort_values(ascending=False)
                plot_feature_importance(importances, f"{model_selected} : Importance des variables")

                # ‚úÖ Tableau des importances
                st.markdown("### üìã Importance des variables")
                st.dataframe(importances.to_frame("Importance"))

        else:
            st.warning("Pas de donn√©es de pr√©diction pour ce mod√®le. Assurez-vous d'utiliser un mod√®le train/test.")

        # Arbre Random Forest
        if "model" in res and model_selected.startswith("Random Forest"):
            rf_model = res["model"]
            st.markdown("### Visualisation d'un arbre de la for√™t")

            n_trees = len(rf_model.estimators_)
            tree_index = st.slider("Choisissez l'arbre √† afficher", 0, n_trees - 1, 0)

            fig, ax = plt.subplots(figsize=(20, 12))
            plot_tree(
                rf_model.estimators_[tree_index],
                feature_names=res.get("features", None),
                filled=True,
                rounded=True,
                fontsize=10,
                ax=ax
            )
            st.pyplot(fig)

            depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]
            fig, ax = plt.subplots()
            sns.histplot(depths, bins=range(max(depths) + 2), ax=ax)
            ax.set_title("Distribution des profondeurs des arbres dans la for√™t")
            ax.set_xlabel("Profondeur de l'arbre")
            ax.set_ylabel("Nombre d'arbres")
            st.pyplot(fig)
    else:
        st.info("Entra√Ænez d'abord un mod√®le pour voir les graphiques.")

# --- Onglet Comparaison ---
with ml_tabs[2]:
    st.subheader("Comparaison des mod√®les")

    if not st.session_state.ml_results:
        st.info("Entra√Ænez d'abord au moins un mod√®le pour comparer.")
    else:
        mod√®les_dispo = list(st.session_state.ml_results.keys())
        mod√®les_choisis = st.multiselect(
            "S√©lectionnez les mod√®les √† comparer", 
            mod√®les_dispo, 
            default=mod√®les_dispo
        )

        if not mod√®les_choisis:
            st.warning("S√©lectionnez au moins un mod√®le pour comparer.")
        else:
            import numpy as np

            # ---------------- R√©sum√© g√©n√©ral (R¬≤, RMSE) ----------------
            r√©sum√© = []
            for m in mod√®les_choisis:
                res = st.session_state.ml_results[m]
                r√©sum√©.append({
                    "Mod√®le": m,
                    "R¬≤ train": res.get("r2_train", res.get("r2", np.nan)),
                    "R¬≤ test": res.get("r2_test", np.nan),
                    "RMSE train": res.get("rmse_train", res.get("rmse", np.nan)),
                    "RMSE test": res.get("rmse_test", np.nan),
                })

            df_resume = pd.DataFrame(r√©sum√©).set_index("Mod√®le")

            # Tableau r√©sum√©
            st.dataframe(df_resume.style.format({
                "R¬≤ train": "{:.3f}",
                "R¬≤ test": "{:.3f}",
                "RMSE train": "{:.3e}",
                "RMSE test": "{:.3e}"
            }))

            # ---------------- Graphiques des m√©triques ----------------
            numeric_cols = df_resume.select_dtypes(include="number").columns
            if not numeric_cols.empty:
                fig, ax = plt.subplots(figsize=(12,6))
                df_resume[numeric_cols].plot(kind="bar", ax=ax)
                plt.ylabel("Score / Erreur")
                plt.title("Comparaison des m√©triques train/test")
                plt.xticks(rotation=45, ha="right")
                plt.tight_layout()
                st.pyplot(fig)

            # ---------------- Coefficients des mod√®les (Ridge uniquement) ----------------
            coeffs_list = []
            for m in mod√®les_choisis:
                res = st.session_state.ml_results[m]
                if "coefficients" in res:  # dispo seulement pour Ridge
                    coef_series = pd.Series(res["coefficients"], name=m)
                    coeffs_list.append(coef_series)

            if coeffs_list:
                df_coeffs = pd.concat(coeffs_list, axis=1).fillna(0)

                st.subheader("üìã Coefficients normalis√©s des variables (Ridge)")
                st.dataframe(df_coeffs.style.format("{:.3e}"))

                st.subheader("üìä Comparaison visuelle des coefficients normalis√©s")
                fig, ax = plt.subplots(figsize=(12,6))
                df_coeffs.plot(kind="bar", ax=ax)
                plt.title("Comparaison des coefficients Ridge par variable (normalis√©s)")
                plt.ylabel("Coefficient normalis√©")
                plt.xticks(rotation=45, ha="right")
                plt.tight_layout()
                st.pyplot(fig)
